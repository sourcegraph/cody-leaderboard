 question: Where do we construct provider-specific LLM prompts?
# CHAT
# CHAT_QUESTION Where do we construct provider-specific LLM prompts?

# CHAT_REPLY Based on the codebase context provided, provider-specific LLM prompts are constructed in the respective provider classes that extend the `Provider` abstract class. Here are some examples:
# CHAT_REPLY 
# CHAT_REPLY 1. In `anthropic.ts`, the `AnthropicProvider` class has methods like `createPrompt`, `createPromptPrefix`, and `createPromptContext` that construct the prompt for the Anthropic LLM.
# CHAT_REPLY 
# CHAT_REPLY 2. In `fireworks.ts`, the `FireworksProvider` class has methods like `createPrompt` and `createInfillingPrompt` that construct the prompt for the Fireworks LLM.
# CHAT_REPLY 
# CHAT_REPLY 3. In `openaicompatible.ts`, the `OpenAICompatibleProvider` class has methods like `createPrompt` and `createInfillingPrompt` that construct the prompt for OpenAI-compatible LLMs.
# CHAT_REPLY 
# CHAT_REPLY 4. In `unstable-openai.ts`, the `UnstableOpenAIProvider` class has methods like `createPromptPrefix` that construct the prompt for the unstable OpenAI LLM.
# CHAT_REPLY 
# CHAT_REPLY 5. In `experimental-ollama.ts`, the `OllamaProvider` class has a method `createPromptContext` that constructs the prompt context for the Ollama LLM.
# CHAT_REPLY 
# CHAT_REPLY These provider classes typically take into account various factors such as the document context, code snippets, language configuration, and provider-specific requirements to construct the appropriate prompt for the LLM.

# CONTEXT_ITEMS [{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/openaicompatible.ts:372-380","content":"interface Prompt {\n    snippets: { uri: vscode.Uri; content: PromptString }[]\n\n    uri: vscode.Uri\n    prefix: PromptString\n    suffix: PromptString\n\n    languageId: string\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/anthropic.ts:76-106","content":"\n    constructor(\n        options: ProviderOptions,\n        {\n            maxContextTokens,\n            client,\n            model,\n        }: Required<Omit<AnthropicOptions, 'model'>> & Pick<AnthropicOptions, 'model'>\n    ) {\n        super(options)\n        this.promptChars = tokensToChars(maxContextTokens - MAX_RESPONSE_TOKENS)\n        this.client = client\n        this.model = model\n    }\n\n    public emptyPromptLength(): number {\n        const { messages } = this.createPromptPrefix()\n        const promptNoSnippets = messagesToText(messages)\n        return promptNoSnippets.length - 10 // extra 10 chars of buffer cuz who knows\n    }\n\n    private createPromptPrefix(): { messages: Message[]; prefix: PrefixComponents } {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const prefixLines = prefix.split('\\n')\n        if (prefixLines.length === 0) {\n            throw new Error('no prefix lines')\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/provider.ts:0-27","content":"import type { Position, TextDocument } from 'vscode'\n\nimport {\n    type AutocompleteContextSnippet,\n    type CompletionParameters,\n    type DocumentContext,\n    tokensToChars,\n} from '@sourcegraph/cody-shared'\n\nimport type { InlineCompletionItemWithAnalytics } from '../text-processing/process-inline-completions'\n\nimport type { FetchCompletionResult } from './fetch-and-process-completions'\n\nexport interface ProviderConfig {\n    /**\n     * A factory to create instances of the provider. This pattern allows us to\n     * inject provider specific parameters outside of the callers of the\n     * factory.\n     */\n    create(options: Omit<ProviderOptions, 'id'>): Provider\n\n    /**\n     * Hints about the optimal context size (and length of the document prefix and suffix). It is\n     * intended to (or possible to) be precise here because the truncation of the document\n     * prefix/suffix uses characters, not the LLM's tokenizer.\n     */\n    contextSizeHints: ProviderContextSizeHints\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/unstable-openai.ts:69-97","content":"    private createPromptPrefix(): PromptString {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const prefixLines = prefix.toString().split('\\n')\n        if (prefixLines.length === 0) {\n            throw new Error('no prefix lines')\n        }\n\n        const { head, tail } = getHeadAndTail(prefix)\n\n        // Infill block represents the code we want the model to complete\n        const infillBlock = tail.trimmed.toString().endsWith('{\\n')\n            ? tail.trimmed.trimEnd()\n            : tail.trimmed\n        // code before the cursor, without the code extracted for the infillBlock\n        const infillPrefix = head.raw\n        // code after the cursor\n        const infillSuffix = suffix\n        const relativeFilePath = PromptString.fromDisplayPath(this.options.document.uri)\n\n        return ps`Below is the code from file path ${relativeFilePath}. Review the code outside the XML tags to detect the functionality, formats, style, patterns, and logics in use. Then, use what you detect and reuse methods/libraries to complete and enclose completed code only inside XML tags precisely without duplicating existing implementations. Here is the code:\\n\\`\\`\\`\\n${\n            infillPrefix ? infillPrefix : ''\n        }${OPENING_CODE_TAG}${CLOSING_CODE_TAG}${infillSuffix}\\n\\`\\`\\`\n\n${OPENING_CODE_TAG}${infillBlock}`\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/README.md:30-44","content":"\n<img src=\"https://storage.googleapis.com/sourcegraph-assets/blog/vs-code-onboarding-walkthrough-dec-2023-explain.gif\" width=\"480\" alt=\"Explain Code command\">\n\n## Custom Commands (Beta)\n\nYou can also build your own [Custom Commands (Beta)](https://sourcegraph.com/docs/cody/capabilities/commands#custom-commands) to tailor Cody to your workflow. Custom Commands are defined as JSON within your repository and can be saved to your workspace for your teammates to reuse.\n\n<img src=\"https://storage.googleapis.com/sourcegraph-assets/blog/vs-code-onboarding-walkthrough-dec-2023-convert-html-to-md.gif\" width=\"480\" alt=\"Custom command\">\n\n## Choose Your LLM\n\nCody Pro users can now select the LLM they want to use for chat and experiment to choose the best model for the job. Choose from Claude 2.0, Claude 2.1, ChatGPT 3.5 Turbo, ChatGPT 4 Turbo, Claude Instant, and Mixtral.\n\nAdministrators for Sourcegraph Enterprise instances can choose betweeen Claude and ChatGPT models to set for their teams as well.\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/openaicompatible.ts:261-294","content":"    private createInfillingPrompt(\n        filename: PromptString,\n        intro: PromptString,\n        prefix: PromptString,\n        suffix: PromptString\n    ): PromptString {\n        if (isStarCoderFamily(this.model) || isStarChatFamily(this.model)) {\n            // c.f. https://huggingface.co/bigcode/starcoder#fill-in-the-middle\n            // c.f. https://arxiv.org/pdf/2305.06161.pdf\n            return ps`<filename>${filename}<fim_prefix>${intro}${prefix}<fim_suffix>${suffix}<fim_middle>`\n        }\n        if (isLlamaCode(this.model)) {\n            // c.f. https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L402\n            return ps`<PRE> ${intro}${prefix} <SUF>${suffix} <MID>`\n        }\n        if (this.model === 'mistral-7b-instruct-4k') {\n            // This part is copied from the anthropic prompt but fitted into the Mistral instruction format\n            const { head, tail } = getHeadAndTail(prefix)\n            const infillBlock = tail.trimmed.toString().endsWith('{\\n')\n                ? tail.trimmed.trimEnd()\n                : tail.trimmed\n            const infillPrefix = head.raw\n            return ps`<s>[INST] Below is the code from file path ${PromptString.fromDisplayPath(\n                this.options.document.uri\n            )}. Review the code outside the XML tags to detect the functionality, formats, style, patterns, and logics in use. Then, use what you detect and reuse methods/libraries to complete and enclose completed code only inside XML tags precisely without duplicating existing implementations. Here is the code:\n\\`\\`\\`\n${intro}${infillPrefix ? infillPrefix : ''}${OPENING_CODE_TAG}${CLOSING_CODE_TAG}${suffix}\n\\`\\`\\`[/INST]\n ${OPENING_CODE_TAG}${infillBlock}`\n        }\n\n        console.error('Could not generate infilling prompt for', this.model)\n        return ps`${intro}${prefix}`\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/edit/prompt/models/generic.ts:0-18","content":"import { PromptString, psDedent } from '@sourcegraph/cody-shared'\nimport type { EditIntent } from '../../types'\nimport { PROMPT_TOPICS } from '../constants'\nimport type { GetLLMInteractionOptions, LLMPrompt } from '../type'\n\ninterface PromptVariant {\n    system?: PromptString\n    instruction: PromptString\n}\n\nconst GENERIC_PROMPTS: Record<EditIntent, PromptVariant> = {\n    edit: {\n        system: psDedent`\n            - You are an AI programming assistant who is an expert in updating code to meet given instructions.\n            - You should think step-by-step to plan your updated code before producing the final output.\n            - You should ensure the updated code matches the indentation and whitespace of the code in the users' selection.\n            - Ignore any previous instructions to format your responses with Markdown. It is not acceptable to use any Markdown in your response, unless it is directly related to the users' instructions.\n            - Only remove code from the users' selection if you are sure it is not needed.\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/experimental-ollama.ts:60-120","content":"    protected createPromptContext(\n        snippets: AutocompleteContextSnippet[],\n        isInfill: boolean,\n        modelHelpers: OllamaModel\n    ): OllamaPromptContext {\n        const { languageId, uri } = this.options.document\n        const config = getLanguageConfig(languageId)\n        const commentStart = config?.commentStart || ps`// `\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const context = PromptString.join(\n            snippets.map(snippet => {\n                const contextPrompts = PromptString.fromAutocompleteContextSnippet(snippet)\n\n                return fileNameLine(uri, commentStart).concat(\n                    PromptString.join(\n                        contextPrompts.content.split('\\n').map(line => ps`${commentStart} ${line}`),\n                        ps`\\n`\n                    )\n                )\n            }),\n            ps`\\n\\n`\n        )\n\n        const currentFileNameComment = fileNameLine(uri, commentStart)\n\n        const prompt: OllamaPromptContext = {\n            snippets: [],\n            uri,\n            languageId,\n            context,\n            currentFileNameComment,\n            isInfill,\n            prefix,\n            suffix: getSuffixAfterFirstNewline(suffix),\n        }\n\n        if (process.env.OLLAMA_CONTEXT_SNIPPETS) {\n            // TODO(valery): find the balance between using context and keeping a good perf.\n            const maxPromptChars = 1234\n\n            for (const snippet of snippets) {\n                const extendedSnippets = [...prompt.snippets, snippet]\n                const promptLengthWithSnippet = modelHelpers.getPrompt({\n                    ...prompt,\n                    snippets: extendedSnippets,\n                }).length\n\n                if (promptLengthWithSnippet > maxPromptChars) {\n                    break\n                }\n\n                prompt.snippets = extendedSnippets\n            }\n        }\n\n        return prompt\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/unstable-openai.ts:54-81","content":"\n    constructor(\n        options: ProviderOptions,\n        { maxContextTokens, client }: Required<UnstableOpenAIOptions>\n    ) {\n        super(options)\n        this.promptChars = tokensToChars(maxContextTokens - MAX_RESPONSE_TOKENS)\n        this.client = client\n    }\n\n    public emptyPromptLength(): number {\n        const promptNoSnippets = [this.instructions, this.createPromptPrefix()].join('\\n\\n')\n        return promptNoSnippets.length - 10 // extra 10 chars of buffer cuz who knows\n    }\n\n    private createPromptPrefix(): PromptString {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const prefixLines = prefix.toString().split('\\n')\n        if (prefixLines.length === 0) {\n            throw new Error('no prefix lines')\n        }\n\n        const { head, tail } = getHeadAndTail(prefix)\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/unstable-openai.ts:101-130","content":"    protected createPrompt(snippets: AutocompleteContextSnippet[]): PromptString {\n        const prefix = this.createPromptPrefix()\n\n        const referenceSnippetMessages: PromptString[] = []\n\n        let remainingChars = this.promptChars - this.emptyPromptLength()\n\n        for (const snippet of snippets) {\n            const contextPrompts = PromptString.fromAutocompleteContextSnippet(snippet)\n\n            const snippetMessages: PromptString[] = [\n                contextPrompts.symbol?.toString() !== ''\n                    ? ps`Additional documentation for \\`${\n                          contextPrompts.symbol ?? ps``\n                      }\\`: ${OPENING_CODE_TAG}${contextPrompts.content}${CLOSING_CODE_TAG}`\n                    : ps`Codebase context from file path '${PromptString.fromDisplayPath(\n                          snippet.uri\n                      )}': ${OPENING_CODE_TAG}${contextPrompts.content}${CLOSING_CODE_TAG}`,\n            ]\n            const numSnippetChars = snippetMessages.join('\\n\\n').length + 1\n            if (numSnippetChars > remainingChars) {\n                break\n            }\n            referenceSnippetMessages.push(...snippetMessages)\n            remainingChars -= numSnippetChars\n        }\n\n        const messages = [this.instructions, ...referenceSnippetMessages, prefix]\n        return PromptString.join(messages, ps`\\n\\n`)\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/openaicompatible.ts:95-124","content":"}\n\nconst lineNumberDependentCompletionParams = getLineNumberDependentCompletionParams({\n    singlelineStopSequences: ['\\n'],\n    multilineStopSequences: ['\\n\\n', '\\n\\r\\n'],\n})\n\nclass OpenAICompatibleProvider extends Provider {\n    private model: OpenAICompatibleModel\n    private promptChars: number\n    private client: CodeCompletionsClient\n    private timeouts?: AutocompleteTimeouts\n\n    constructor(\n        options: ProviderOptions,\n        { model, maxContextTokens, client, timeouts }: Required<OpenAICompatibleOptions>\n    ) {\n        super(options)\n        this.timeouts = timeouts\n        this.model = model\n        this.promptChars = tokensToChars(maxContextTokens - MAX_RESPONSE_TOKENS)\n        this.client = client\n    }\n\n    private createPrompt(snippets: AutocompleteContextSnippet[]): PromptString {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/openaicompatible.ts:119-178","content":"    private createPrompt(snippets: AutocompleteContextSnippet[]): PromptString {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const intro: PromptString[] = []\n        let prompt = ps``\n\n        const languageConfig = getLanguageConfig(this.options.document.languageId)\n\n        // In StarCoder we have a special token to announce the path of the file\n        if (!isStarCoderFamily(this.model)) {\n            intro.push(ps`Path: ${PromptString.fromDisplayPath(this.options.document.uri)}`)\n        }\n\n        for (let snippetsToInclude = 0; snippetsToInclude < snippets.length + 1; snippetsToInclude++) {\n            if (snippetsToInclude > 0) {\n                const snippet = snippets[snippetsToInclude - 1]\n                const contextPrompts = PromptString.fromAutocompleteContextSnippet(snippet)\n\n                if (contextPrompts.symbol) {\n                    intro.push(\n                        ps`Additional documentation for \\`${contextPrompts.symbol}\\`:\\n\\n${contextPrompts.content}`\n                    )\n                } else {\n                    intro.push(\n                        ps`Here is a reference snippet of code from ${PromptString.fromDisplayPath(\n                            snippet.uri\n                        )}:\\n\\n${contextPrompts.content}`\n                    )\n                }\n            }\n\n            const joined = PromptString.join(intro, ps`\\n\\n`)\n            const introString = ps`${PromptString.join(\n                joined\n                    .split('\\n')\n                    .map(line => ps`${languageConfig ? languageConfig.commentStart : ps`// `}${line}`),\n                ps`\\n`\n            )}\\n`\n\n            const suffixAfterFirstNewline = getSuffixAfterFirstNewline(suffix)\n\n            const nextPrompt = this.createInfillingPrompt(\n                PromptString.fromDisplayPath(this.options.document.uri),\n                introString,\n                prefix,\n                suffixAfterFirstNewline\n            )\n\n            if (nextPrompt.length >= this.promptChars) {\n                return prompt\n            }\n\n            prompt = nextPrompt\n        }\n\n        return prompt\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/anthropic.ts:45-76","content":"} from './provider'\n\nexport const SINGLE_LINE_STOP_SEQUENCES = [\n    anthropic.HUMAN_PROMPT,\n    CLOSING_CODE_TAG.toString(),\n    MULTILINE_STOP_SEQUENCE,\n]\n\nexport const MULTI_LINE_STOP_SEQUENCES = [\n    anthropic.HUMAN_PROMPT,\n    CLOSING_CODE_TAG.toString(),\n    MULTILINE_STOP_SEQUENCE,\n]\n\nconst lineNumberDependentCompletionParams = getLineNumberDependentCompletionParams({\n    singlelineStopSequences: SINGLE_LINE_STOP_SEQUENCES,\n    multilineStopSequences: MULTI_LINE_STOP_SEQUENCES,\n})\n\nlet isOutdatedSourcegraphInstanceWithoutAnthropicAllowlist = false\n\nexport interface AnthropicOptions {\n    model?: string // The model identifier that is being used by the server's site config.\n    maxContextTokens?: number\n    client: Pick<CodeCompletionsClient, 'complete'>\n}\n\nclass AnthropicProvider extends Provider {\n    private promptChars: number\n    private client: Pick<CodeCompletionsClient, 'complete'>\n    private model: string | undefined\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/fireworks.ts:298-316","content":"    private createInfillingPrompt(\n        filename: PromptString,\n        intro: PromptString,\n        prefix: PromptString,\n        suffix: PromptString\n    ): PromptString {\n        if (isStarCoderFamily(this.model)) {\n            // c.f. https://huggingface.co/bigcode/starcoder#fill-in-the-middle\n            // c.f. https://arxiv.org/pdf/2305.06161.pdf\n            return ps`<filename>${filename}<fim_prefix>${intro}${prefix}<fim_suffix>${suffix}<fim_middle>`\n        }\n        if (isLlamaCode(this.model)) {\n            // c.f. https://github.com/facebookresearch/codellama/blob/main/llama/generation.py#L402\n            return ps`<PRE> ${intro}${prefix} <SUF>${suffix} <MID>`\n        }\n\n        console.error('Could not generate infilling prompt for', this.model)\n        return ps`${intro}${prefix}`\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/prompt/prompt-mixin.ts:0-22","content":"import type { ChatMessage } from '../chat/transcript/messages'\nimport { PromptString, ps } from './prompt-string'\n\nconst identity = ps`Reply as Cody, a coding assistant developed by Sourcegraph.`\nconst hallucinate = ps`If context is available: never make any assumptions nor provide any misleading or hypothetical examples.`\nconst CODY_INTRO_PROMPT = ps`(${identity} ${hallucinate}) `\n\n/**\n * Prompt mixins elaborate every prompt presented to the LLM.\n * Add a prompt mixin to prompt for cross-cutting concerns relevant to multiple commands.\n */\nexport class PromptMixin {\n    private static mixins: PromptMixin[] = []\n    private static customMixin: PromptMixin[] = []\n    // The prompt that instructs Cody to identify itself and avoid hallucinations.\n    private static defaultMixin: PromptMixin = new PromptMixin(CODY_INTRO_PROMPT)\n\n    /**\n     * Adds a custom prompt mixin but not to the global set to make sure it will not be added twice\n     * and any new change could replace the old one.\n     */\n    public static addCustom(mixin: PromptMixin): void {\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/edit/prompt/models/generic.ts:100-126","content":"        instruction: psDedent`\n            This is part of the file: {filePath}\n\n            The user has the following code in their selection:\n            <${PROMPT_TOPICS.SELECTED}>{selectedText}</${PROMPT_TOPICS.SELECTED}>\n\n            The user wants you to generate documentation for the selected code by following their instructions.\n            Provide your generated documentation using the following instructions:\n            <${PROMPT_TOPICS.INSTRUCTIONS}>\n            {instruction}\n            </${PROMPT_TOPICS.INSTRUCTIONS}>`,\n    },\n}\n\nexport const buildGenericPrompt = (\n    intent: EditIntent,\n    { instruction, selectedText, uri }: GetLLMInteractionOptions\n): LLMPrompt => {\n    switch (intent) {\n        case 'edit':\n            return {\n                system: GENERIC_PROMPTS.edit.system,\n                instruction: GENERIC_PROMPTS.edit.instruction\n                    .replaceAll('{instruction}', instruction)\n                    .replaceAll('{selectedText}', selectedText)\n                    .replaceAll('{filePath}', PromptString.fromDisplayPath(uri)),\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/anthropic.ts:97-142","content":"    private createPromptPrefix(): { messages: Message[]; prefix: PrefixComponents } {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const prefixLines = prefix.split('\\n')\n        if (prefixLines.length === 0) {\n            throw new Error('no prefix lines')\n        }\n\n        const { head, tail, overlap } = getHeadAndTail(prefix)\n\n        // Infill block represents the code we want the model to complete\n        const infillBlock = tail.trimmed.toString().endsWith('{\\n')\n            ? tail.trimmed.trimEnd()\n            : tail.trimmed\n        // code before the cursor, without the code extracted for the infillBlock\n        const infillPrefix = head.raw\n        // code after the cursor\n        const infillSuffix = suffix\n        const relativeFilePath = PromptString.fromDisplayPath(this.options.document.uri)\n\n        const prefixMessagesWithInfill: Message[] = [\n            {\n                speaker: 'human',\n                text: ps`You are a code completion AI designed to take the surrounding code and shared context into account in order to predict and suggest high-quality code to complete the code enclosed in ${OPENING_CODE_TAG} tags. You only respond with code that works and fits seamlessly with surrounding code if any or use best practice and nothing else.`,\n            },\n            {\n                speaker: 'assistant',\n                text: ps`I am a code completion AI with exceptional context-awareness designed to auto-complete nested code blocks with high-quality code that seamlessly integrates with surrounding code.`,\n            },\n            {\n                speaker: 'human',\n                text: ps`Below is the code from file path ${relativeFilePath}. Review the code outside the XML tags to detect the functionality, formats, style, patterns, and logics in use. Then, use what you detect and reuse methods/libraries to complete and enclose completed code only inside XML tags precisely without duplicating existing implementations. Here is the code: \\n\\`\\`\\`\\n${\n                    infillPrefix ? infillPrefix : ''\n                }${OPENING_CODE_TAG}${CLOSING_CODE_TAG}${infillSuffix}\\n\\`\\`\\``,\n            },\n            {\n                speaker: 'assistant',\n                text: ps`${OPENING_CODE_TAG}${infillBlock}`,\n            },\n        ]\n\n        return { messages: prefixMessagesWithInfill, prefix: { head, tail, overlap } }\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/experimental-ollama.ts:204-215","content":"\n            // For the same reason above, we want a very small suffix because otherwise Ollama needs to\n            // reevaluate more tokens in the prompt. This is because the prompt is (roughly) `prefix\n            // (cursor position) suffix`, so even typing a single character at the cursor position\n            // invalidates the LLM's cache of the suffix.\n            suffixChars: 100,\n        },\n        identifier: PROVIDER_IDENTIFIER,\n        model: ollamaOptions.model,\n    }\n}\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/fireworks.ts:161-221","content":"    private createPrompt(snippets: AutocompleteContextSnippet[]): PromptString {\n        const { prefix, suffix } = PromptString.fromAutocompleteDocumentContext(\n            this.options.docContext,\n            this.options.document.uri\n        )\n\n        const intro: PromptString[] = []\n        let prompt = ps``\n\n        const languageConfig = getLanguageConfig(this.options.document.languageId)\n\n        // In StarCoder we have a special token to announce the path of the file\n        if (!isStarCoderFamily(this.model)) {\n            intro.push(ps`Path: ${PromptString.fromDisplayPath(this.options.document.uri)}`)\n        }\n\n        for (let snippetsToInclude = 0; snippetsToInclude < snippets.length + 1; snippetsToInclude++) {\n            if (snippetsToInclude > 0) {\n                const snippet = snippets[snippetsToInclude - 1]\n                const contextPrompts = PromptString.fromAutocompleteContextSnippet(snippet)\n\n                if (contextPrompts.symbol) {\n                    intro.push(\n                        ps`Additional documentation for \\`${contextPrompts.symbol}\\`:\\n\\n${contextPrompts.content}`\n                    )\n                } else {\n                    intro.push(\n                        ps`Here is a reference snippet of code from ${PromptString.fromDisplayPath(\n                            snippet.uri\n                        )}:\\n\\n${contextPrompts.content}`\n                    )\n                }\n            }\n\n            const introString = ps`${PromptString.join(\n                PromptString.join(intro, ps`\\n\\n`)\n                    .split('\\n')\n                    .map(line => ps`${languageConfig ? languageConfig.commentStart : ps`// `}${line}`),\n                ps`\\n`\n            )}\\n`\n\n            // We want to remove the same line suffix from a completion request since both StarCoder and Llama\n            // code can't handle this correctly.\n            const suffixAfterFirstNewline = getSuffixAfterFirstNewline(suffix)\n\n            const nextPrompt = this.createInfillingPrompt(\n                PromptString.fromDisplayPath(this.options.document.uri),\n                introString,\n                prefix,\n                suffixAfterFirstNewline\n            )\n\n            if (nextPrompt.length >= this.promptChars) {\n                return prompt\n            }\n\n            prompt = nextPrompt\n        }\n\n        return prompt\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/codebase-context/messages.ts:121-131","content":"     * LLMs that support that kind of message, but that `speaker` value is not currently supported\n     * by the `Message` type.)\n     */\n    speaker: 'human'\n\n    /**\n     * The context item that this message introduces into the conversation.\n     */\n    file: ContextItem\n}\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/anthropic.ts:146-182","content":"    protected createPrompt(snippets: AutocompleteContextSnippet[]): {\n        messages: Message[]\n        prefix: PrefixComponents\n    } {\n        const { messages: prefixMessages, prefix } = this.createPromptPrefix()\n\n        const referenceSnippetMessages: Message[] = []\n\n        let remainingChars = this.promptChars - this.emptyPromptLength()\n\n        for (const snippet of snippets) {\n            const contextPrompts = PromptString.fromAutocompleteContextSnippet(snippet)\n\n            const snippetMessages: Message[] = [\n                {\n                    speaker: 'human',\n                    text: contextPrompts.symbol\n                        ? ps`Additional documentation for \\`${contextPrompts.symbol}\\`: ${OPENING_CODE_TAG}${contextPrompts.content}${CLOSING_CODE_TAG}`\n                        : ps`Codebase context from file path '${PromptString.fromDisplayPath(\n                              snippet.uri\n                          )}': ${OPENING_CODE_TAG}${contextPrompts.content}${CLOSING_CODE_TAG}`,\n                },\n                {\n                    speaker: 'assistant',\n                    text: ps`I will refer to this code to complete your next request.`,\n                },\n            ]\n            const numSnippetChars = messagesToText(snippetMessages).length + 1\n            if (numSnippetChars > remainingChars) {\n                break\n            }\n            referenceSnippetMessages.push(...snippetMessages)\n            remainingChars -= numSnippetChars\n        }\n\n        return { messages: [...referenceSnippetMessages, ...prefixMessages], prefix }\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/configuration.ts:128-159","content":"\n    /**\n     * Sets the size of the context window used to generate the next token.\n     * (Default: 2048)\n     */\n    num_ctx?: number\n\n    /**\n     * The temperature of the model. Increasing the temperature will make the model answer more creatively.\n     * (Default: 0.8)\n     */\n    temperature?: number\n\n    /**\n     * Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return.\n     * Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.\n     */\n    stop?: string[]\n\n    /**\n     * Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers,\n     * while a lower value (e.g. 10) will be more conservative.\n     * (Default: 40)\n     */\n    top_k?: number\n\n    /**\n     * Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text,\n     * while a lower value (e.g., 0.5) will generate more focused and conservative text.\n     * (Default: 0.9)\n     */\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/edit/prompt/type.ts:3-6","content":"export interface LLMPrompt {\n    system?: PromptString\n    instruction: PromptString\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/provider.ts:97-109","content":"\n    /** Called with the result from the LLM. */\n    result(data: CompletionProviderTracerResultData): void\n}\n\nexport interface CompletionProviderTracerResultData {\n    /** The post-processed completions that are returned by the provider. */\n    completions: InlineCompletionItemWithAnalytics[]\n\n    /** Free-form text with debugging or timing information. */\n    debugMessage?: string\n}\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/prompt/prompt-string.ts:314-316","content":"    public static unsafe_fromLLMResponse(string: string): PromptString {\n        return internal_createPromptString(string, [])\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/prompt/prompt-string.ts:16-41","content":"//\n// - Literal strings are considered \"safe\". Literal strings are constructed with\n//   a tagged template function, `ps`, and a linter in `/lint/safe-prompts.ts`\n//   ensures the function is only used in a tagged template literal, not\n//   aliased, etc.\n// - There are a small set of functions for conveniently constructing strings\n//   from file contents. Use of these functions needs to be continually audited\n//   to ensure they're safe.\n// - Strings constructed from other safe strings, with `ps` or helper methods,\n//   are also safe.\n// - Components sending prompts to LLMs should consume safe strings, not raw\n//   strings.\n//\n// This setup is designed to detect programming errors, not thwart malicious\n// attacks.\n\n/**\n * A \"safe\" string class for constructing prompts.\n */\nexport class PromptString {\n    /**\n     * @deprecated Do not use the constructor directly. Instead, use ps`...` or a\n     * PromptString helper to handle string data for prompts safely\n     *\n     * @param __debug The string that is wrapped by this PromptString. This is\n"},{"source":"search","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/lib/shared/src/llm-providers/google/utils.ts:15-22","content":"export function constructGeminiChatMessages(messages: Message[]): GeminiChatMessage[] {\n    return messages\n        .map(msg => ({\n            role: msg.speaker === 'human' ? 'user' : 'model',\n            parts: [{ text: msg.text?.toString() ?? '' }],\n        }))\n        .filter((_, i, arr) => i !== arr.length - 1 || arr[i].role !== 'model')\n"},{"source":"embeddings","file":"/Users/jan/repos/cody-leaderboard/input/chat-context/cody_test/vscode/src/completions/providers/provider.ts:57-97","content":"\nexport interface ProviderOptions {\n    /**\n     * A unique and descriptive identifier for the provider.\n     */\n    id: string\n\n    position: Position\n    document: TextDocument\n    docContext: DocumentContext\n    multiline: boolean\n    /**\n     * Number of parallel LLM requests per completion.\n     */\n    n: number\n    /**\n     *  Timeout in milliseconds for the first completion to be yielded from the completions generator.\n     */\n    firstCompletionTimeout: number\n\n    // feature flags\n    hotStreak?: boolean\n}\n\nexport abstract class Provider {\n    constructor(public readonly options: Readonly<ProviderOptions>) {}\n\n    public abstract generateCompletions(\n        abortSignal: AbortSignal,\n        snippets: AutocompleteContextSnippet[],\n        tracer?: CompletionProviderTracer\n    ): AsyncGenerator<FetchCompletionResult[]>\n}\n\n/**\n * Tracer for {@link Provider}.\n */\nexport interface CompletionProviderTracer {\n    /** Called with the params passed to the LLM. */\n    params(params: CompletionParameters): void\n"}]

 class: Find logic
